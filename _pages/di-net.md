---
permalink: /di-net
title: Discretization Invariant Networks
---

I am a PhD candidate at MIT CSAIL advised by [Polina Golland](https://people.csail.mit.edu/polina/). My research focuses on medical vision, with topics that include neural fields and robust or interpretable deep learning. I also built models for trajectory estimation in colonoscopies at [Iterative Scopes](https://www.iterativescopes.com/). I am supported by the [Takeda Fellowship](https://mittakedaprogram.mit.edu/) and [Siebel Scholarship](http://www.siebelscholars.com/).

I previously worked with [Jim Duncan](https://medicine.yale.edu/profile/james_duncan/) and [Julius Chapiro](https://medicine.yale.edu/profile/julius_chapiro/) in the [Yale Radiology Research Lab](https://medicine.yale.edu/lab/radresearch/), building interpretable neural networks for liver cancer diagnosis. I received a B.S. in Biomedical Engineering at Yale, where I developed computational models of heart muscle contraction under [Stuart Campbell](https://seas.yale.edu/faculty-research/faculty-directory/stuart-campbell).


<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
}

@media screen and (min-width: 980px){
    body {
        width: 980px;
    }
}

h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #5364cc;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1 {
    text-align: center;
}
h2,h3 {
    text-align: left;
}

h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
h3 {
    font-weight: 600;
    margin: 16px 0px 4px 0px;
}

.paper-title {
    padding: 1px 0px 1px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.col-1 {
     width: 100%;
     float: left;
}

.author-row, .affil-row {
    font-size: 26px;
}

.author-row-new { 
    text-align: center; 
}

.author-row-new a {
    display: inline-block;
    font-size: 20px;
    padding: 4px;
}

.author-row-new sup {
    color: #313436;
    font-size: 12px;
}

.affiliations-new {
    font-size: 18px;
    text-align: center;
    width: 80%;
    margin: 0 auto;
    margin-bottom: 20px;
}

.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 26px;
}
.affiliatons {
    font-size: 18px;
}
.affil-row {
    margin-top: 18px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    color: #666;
    margin-top: 4px;
    margin-bottom: 10px;
}


video {
    display: block;
    margin: auto;
}


figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    padding: 0;
    margin: 0;
    list-style: none;
}

.paper-btn-coming-soon {
    position: relative; 
    top: 0;
    left: 0;
}

.coming-soon {
    position: absolute;
    top: -15px;
    right: -15px;
}

.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #5364cc;
  color: white !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}
.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.paper-btn:hover {
    opacity: 0.85;
}

.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}

.venue {
    font-size: 23px;
}

.topnav {
    background-color: #EEEEEE;
    overflow: hidden;
}

.topnav div {
    max-width: 1070px;
    margin: 0 auto;
}

.topnav a {
    display: inline-block;
    color: black;
    text-align: center;
    vertical-align: middle;
    padding: 16px 16px;
    text-decoration: none;
    font-size: 18px;
}

.topnav img {
    padding: 2px 0px;
    width: 100%;
    margin: 0.2em 0px 0.3em 0px;
    vertical-align: middle;
}

pre {
    font-size: 0.9em;
    padding-left: 7px;
    padding-right: 7px;
    padding-top: 3px;
    padding-bottom: 3px;
    border-radius: 3px;
    background-color: rgb(235, 235, 235);
    overflow-x: auto;
}

.download-thumb {
    display: flex;
}

@media only screen and (max-width: 620px) {
    .download-thumb {
        display: none;
    }
}

.paper-stuff {
    width: 50%;
    font-size: 20px;
}

@media only screen and (max-width: 620px) {
    .paper-stuff {
        width: 100%;
    }
}
* {
  box-sizing: border-box;
}

.column {
  text-align: center;
  float: left;
  width: 16.666%;
  padding: 5px;
}
.column3 {
  text-align: center;
  float: left;
  width: 33.333%;
  padding: 5px;
}
.column4 {
  text-align: center;
  float: left;
  width: 50%;
  padding: 5px;
}
.column5 {
  text-align: center;
  float: left;
  width: 20%;
  padding: 5px;
}
.border-right {
    border-right: 1px solid black;
}
.border-bottom{
    border-bottom: 1px solid black;
}



/* Clearfix (clear floats) */
.row::after {
  content: "";
  clear: both;
  display: table;
}
.img-fluid {
  max-width: 100%;
  height: auto;
}
.figure-img {
  margin-bottom: 0.5rem;
  line-height: 1;
}





.rounded-circle {
  border-radius: 50% !important;
}



/* Responsive layout - makes the three columns stack on top of each other instead of next to each other */
@media screen and (max-width: 500px) {
  .column {
    width: 100%;
  }
}
@media screen and (max-width: 500px) {
  .column3 {
    width: 100%;
  }
}

</style>
<link rel="stylesheet" href="bootstrap-grid.css">

<script type="text/javascript" src="../js/hidebib.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
    <head>
        <title> Composing Ensembles of Pre-trained Models via Iterative Consensus</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta property="og:description" content="Composing Ensembles of Pre-trained Models via Iterative Consensus"/>
        <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:creator" content="@karsten_kreis">
        <meta name="twitter:title" content="Composing Ensembles of Pre-trained Models via Iterative Consensus">
        <meta name="twitter:description" content="">
        <meta name="twitter:image" content="">
    </head>

 <body>


<div class="container">
    <div class="paper-title">
    <h1> 
        Composing Ensembles of Pre-trained Models via Iterative Consensus
    </div>

    <div id="authors">
        <center>
            <div class="author-row-new">
                <a href="https://people.csail.mit.edu/lishuang/">Shuang Li*<sup>1</sup></a>,
                <a href="https://yilundu.github.io/">Yilun Du*<sup>1</sup></a>,
                <a href="https://scholar.google.com/citations?user=rRJ9wTJMUB8C&hl=en">Joshua B. Tenenbaum<sup>1</sup></a>,
                <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba<sup>1</sup></a>
                <a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en">Igor Mordatch<sup>2</sup></a>
            </div>
        </center>
        <center>
        <div class="affiliations">
            <span><sup>1</sup> MIT</span>
            <span><sup>2</sup> Google Brain</span><br/>
        </div>

        <br>*indicates equal contribution. Shuang Li did all the experiments on image generation, video question answering, and mathematical reasoning. Yilun Du did all the experiments on robot manipulation.

        <div class="affil-row">
            <div class="venue text-center"><b>arXiv 2022 </b></div>
        </div>

        </center>

        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="paper-btn" href="https://arxiv.org/abs/2210.11522">
                <span class="material-icons"> description </span> 
                 Paper
            </a>
            <div class="paper-btn-coming-soon">
                <!-- https://github.com/ShuangLI59/Composing-Ensembles-of-Pre-trained-Models-via-Iterative-Consensus -->
                <a class="paper-btn" href="">
                    <span class="material-icons"> code </span>
                    Code
                </a>
            </div>
        </div></div>
    </div>

    
    <!-- <section id="teaser-image">
        <center>
            <figure>
                <video class="centered" width="80%" autoplay loop muted playsinline class="video-background " >
                    <source src="assets/LION_video_v10.mp4#t=0.001" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>

        </center>
    </section>
     -->


    <br>
    
    <section id="teaser-image">
        <center>
            <center><p><b>A unified framework for composing pre-trained models.</b></p></center>

            <figure>
                <video width="650" loop autoplay muted>
                    <source src="materials/teaser-2.mp4" type="video/mp4">
                </video>

                <br><br>
                
                <video width="960" loop autoplay muted>
                    <source src="materials/all_results.mp4" type="video/mp4">
                </video>

                <!-- <br><br> -->

                <!-- <video width="800" loop autoplay muted style="border:1px solid black">
                    <source src="materials/new3.mp4" type="video/mp4">
                </video>
 -->
            </figure>

        </center>
    </section>
    
    

    
    <section id="abstract"/>
        <hr>
        <h2>Abstract</h2>
        <div class="flex-row">
            <p>
                Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. 
                <br><br>
                In this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as "generators" or "scorers" and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation.
            </p>
        </div>
    </section>
    <section id="method"/>
        <hr>
        <h2>Method</h2>
            
            <div class="mx-auto">
                <left><p>The proposed framework that composes a "generator" and an ensemble of "scorers" through iterative consensus enables zero-shot generalization across a variety of multimodal tasks.</p></left>
                <center><img class="card-img-top" src="materials/teaser.png" style="width:950px"></center>
            </div>


            <br><br><br>

            <!-- <div class="row">
                    <div class="column4">
                        <center><img class="card-img-top" src="materials/framework.png" style="width:400px"></center>
                    </div>

                    <div class="column4">
                        <video width="400" loop autoplay muted>
                            <source src="materials/teaser3.mp4" type="video/mp4">
                        </video>
                    </div>
            </div>
 -->



            <div class="flex-row">
                <div class="mx-auto">
                    <left><p><b>Overview of the proposed unified framework.</b> Dashed lines are omitted for certain tasks. Orange lines represent the components used to refine the generated result.</p></left>
                    <br>
                    <center><img class="card-img-top" src="materials/framework.png" style="width:400px"></center>

                    <br><br>

                    <!-- <video width="800" loop autoplay muted style="border:1px solid black"> -->
                    <video width="850" loop autoplay muted controls>
                        <source src="materials/new3-2.mp4" type="video/mp4">
                    </video>
                </div>

                <br><br>
                <p><b>Image generation: </b> A pre-trained diffusion model is used as the generator, and multiple scorers, such as CLIP and image classifiers, are used to provide feedback to the generator.</p>
                <p><b>Video question answering: </b> GPT-2 is used as the generator, and a set of CLIP models are used as scorers.</p>
                <p><b>Grade school math: </b> GPT-2 is used as the generator, and a set of question-solution classifiers are used as scorers.</p>
                <p><b>Robot manipulation: </b> MPC+World model is used as the generator, and a pre-trained image segmentation model is used to compute the scores from multiple camera views to select the best action.</p>

            </div>

    </section>




        

    <section id="results">
        <hr>
        <h2>Video Question Answering Results</h2>  
            <center>
            <figure>
                <video class="centered" width="100%" autoplay loop muted playsinline class="video-background " >
                    <source src="materials/vqa.mp4#t=0.001" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
            </center>

        <hr>


        <h2>Grade School Math Results</h2>  

            <center>
            <figure>
                <video class="centered" width="100%" autoplay loop muted playsinline class="video-background " >
                    <source src="materials/math.mp4#t=0.001" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
            </center>

        <hr>

        <h2>Image Generation</h2>  

            <center>
            <figure>
                <video class="centered" width="100%" autoplay loop muted playsinline class="video-background " >
                    <source src="materials/image_generation.mp4#t=0.001" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
            </center>

        <hr>


        <h2>Robot Manipulation Results</h2>  

            <center>
            <figure>
                <video class="centered" width="100%" autoplay loop muted playsinline class="video-background " >
                    <source src="materials/robot_faster.mp4#t=0.001" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
            </figure>
            </center>

    </section> 



    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>@inproceedings{zeng2022lion,
            title={LION: Latent Point Diffusion Models for 3D Shape Generation},
            author={Xiaohui Zeng and Arash Vahdat and Francis Williams and Zan Gojcic and Or Litany and Sanja Fidler and Karsten Kreis},
            booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
            year={2022}
        }</code></pre>
    </section>

    <section>
        This webpage template was recycled from <a href='https://nv-tlabs.github.io/LION/'>here</a>.
        <center><p><a href='https://accessibility.mit.edu/'><b>Accessibility</b></a></p></center>
    </section>
    


</div>
</body>

<body>
<div class='section_div' id="papers">

<h2>Publications</h2>

<table class="pub_table">
 
<tr>
  <td class="pub_td1"><div class="teaser_img_div"><a href="http://arxiv.org/abs/2206.01178"><img class="teaser_img" src="images/2022_inrnet.png" /></a></div></td>
  <td class="pub_td2"><b>Approximate Discretization Invariance for Deep Learning on Implicit Neural Datasets</b><br />
		<div class='paper_metadata'>
  	<u>Clinton Wang</u>,
  	<a href="https://people.csail.mit.edu/polina/">Polina Golland</a><br />
  	<a href="https://www.neurreps.org/"><i>NeurIPS Workshop on Symmetry and Geometry in Neural Representations</i></a> 2022<br />
  	</div>
  <a href="http://arxiv.org/abs/2206.01178">Paper</a> | <a href="https://github.com/clintonjwang/DI-net">Code</a>
</td></tr>

<!-- 
<tr>
  <td class="pub_td1"><div class="teaser_img_div"><a href=""><img class="teaser_img" src="images/tmi22_teaser.png" /></a></div></td>
  <td class="pub_td2"><b>High Fidelity Medical Image-to-Image Translation with Spatial-Intensity Transforms</b><br />
		<div class='paper_metadata'>
  	<u>Clinton Wang</u>,
  	<a href="https://www.massgeneral.org/doctors/17477/natalia-rost/">Natalia Rost</a>,
  	<a href="https://people.csail.mit.edu/polina/">Polina Golland</a><br />
  	<i>arXiv preprint</i> 2022<br />
  	</div>
  <a href="">Paper</a> | <a href="https://github.com/clintonjwang/spatial-intensity-transforms">Code</a> 
</td></tr>
-->

<tr>
  <td class="pub_td1"><div class="teaser_img_div"><a href="http://arxiv.org/abs/2208.02895"><img class="teaser_img" src="images/placenta_teaser.png" /></a></div></td>
  <td class="pub_td2"><b>Automatic Segmentation of the Placenta in BOLD MRI Time Series</b><br />
		<div class='paper_metadata'>
  	<a href="http://people.csail.mit.edu/abulnaga/">Mazdak Abulnaga</a>,
  	<a href="https://connects.catalyst.harvard.edu/Profiles/display/Person/194239">Sean Young</a>, Katherine Hobgood, Eileen Pan, <u>Clinton Wang</u>,
  	<a href="https://www.childrenshospital.org/directory/patricia-ellen-grant">Ellen Grant</a>,
  	<a href="https://www.childrenshospital.org/research/researchers/esra-abaci-turk">Esra Abaci Turk</a>,
  	<a href="https://people.csail.mit.edu/polina/">Polina Golland</a><br />
  	<a href="https://pippiworkshop.github.io/"><i>Medical Image Computing and Computer Assisted Intervention PIPPI Workshop</i></a> 2022<br />
  	</div>
  <a href="http://arxiv.org/abs/2208.02895">Paper</a> | <a href="https://github.com/mabulnaga/automatic-placenta-segmentation">Code</a>
</td></tr>

<tr>
  <td class="pub_td1"><div class="teaser_img_div"><a href="https://doi.org/10.1007/978-3-030-59713-9_72"><img class="teaser_img" src="images/miccai20_teaser.png" /></a></div></td>
  <td class="pub_td2"><b>Spatial-Intensity Transform GANs for High Fidelity Medical Image-to-Image Translation</b><br />
		<div class='paper_metadata'>
  	<u>Clinton Wang</u>,
  	<a href="https://www.massgeneral.org/doctors/17477/natalia-rost/">Natalia Rost</a>,
  	<a href="https://people.csail.mit.edu/polina/">Polina Golland</a><br />
  	<a href="http://www.miccai.org/"><i>Medical Image Computing and Computer Assisted Intervention</i></a> 2020<br />
  	</div>
  <a href="https://doi.org/10.1007/978-3-030-59713-9_72">Paper</a> | <a href="https://drive.google.com/file/d/1Ckaja6Xm8o25zjhfT6DLkXJAgxAEMFEF/view?usp=sharing">Talk</a> | <a href="files/miccai20_talk.pptx">Slides</a> | <a href="https://github.com/clintonjwang/spatial-intensity-transforms">Code</a> 
</td></tr>

<tr>
  <td class="pub_td1">
		<a href="https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/">
			<video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" style="border:1px solid black">
  		<source src="files/21-virtualhome-language.mp4" type="video/mp4">
			</video>
		</a>
	</td>
  <td class="pub_td2"><b>Pre-Trained Language Models for Interactive Decision-Making</b><br />
		<div class='paper_metadata'>
  	<a href="https://people.csail.mit.edu/lishuang/">Shuang Li</a>,
		<a href="https://people.csail.mit.edu/xavierpuig/">Xavier Puig</a>,
		<a href="https://cpaxton.github.io/about/">Chris Paxton</a>,
		<a href="https://yilundu.github.io/">Yilun Du</a>,
		<u>Clinton Wang</u>,
		<a href="https://scholar.google.com/citations?user=sljtWIUAAAAJ&hl=en">Linxi Fan</a>,
		<a href="https://taochenshh.github.io">Tao Chen</a>,
		<a href="https://ai.stanford.edu/~dahuang/">De-An Huang</a>,
		<a href="https://www.ekinakyurek.me/">Ekin Akyürek</a>, 
		<a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>,
		<a href="https://www.mit.edu/~jda/">Jacob Andreas</a>,
		<a href="https://scholar.google.com/citations?user=Vzr1RukAAAAJ&hl=en">Igor Mordatch</a>, 
		<a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>,
		<a href="https://www.cs.utexas.edu/~yukez/">Yuke Zhu</a><br />
	  <a href="https://neurips.cc/Conferences/2022"><i>NeurIPS</i></a> 2022<br />
		</div>
  <a href="https://arxiv.org/abs/2202.01771">Paper</a> | <a href="https://shuangli-project.github.io/Pre-Trained-Language-Models-for-Interactive-Decision-Making/">Project</a> | <a href="https://github.com/ShuangLI59/Pre-Trained-Language-Models-for-Interactive-Decision-Making">Code</a> 
</td></tr>

<tr>
  <td class="pub_td1"><div class="teaser_img_div"><a href="https://doi.org/10.1007/s00330-020-07559-1"><img class="teaser_img" src="images/2021_eurorad_paula.png" /></a></div></td>
  <td class="pub_td2"><b>Deep learning–assisted differentiation of pathologically proven atypical and typical hepatocellular carcinoma (HCC) versus non-HCC on contrast-enhanced MRI of the liver</b><br />
		<div class='paper_metadata'>
	  Paula Oestmann, <u>Clinton Wang</u>,
	  <a href="https://radiologie.charite.de/en/metas/person/person/address_detail/savic/">Lynn Savic</a>, Charlie Hamm, Sophie Stark, Isabel Schobert,
	  <a href="https://radiologie.charite.de/metas/person/person/address_detail/gebauer-6/">Bernhard Gebauer</a>,
		<a href="https://medicine.yale.edu/profile/todd_schlachter/">Todd Schlachter</a>,
		<a href="https://medicine.yale.edu/profile/mingde_lin/">MingDe Lin</a>,
		<a href="https://medicine.yale.edu/profile/jeffrey_weinreb/">Jeffrey Weinreb</a>,
		Ramesh Batra, David Mulligan, Xuchen Zhang,
		<a href="https://medicine.yale.edu/profile/james_duncan/">James Duncan</a>,
		<a href="https://medicine.yale.edu/profile/julius_chapiro/">Julius Chapiro</a><br />
	  <a href="https://www.springer.com/journal/330"><i>European Radiology</i></a> 2021<br />
		</div>
  <a href="https://doi.org/10.1007/s00330-020-07559-1">Paper</a> | <a href="https://github.com/clintonjwang/voi-classifier">Code</a>
</td></tr>

<tr>
  <td class="pub_td1"><div class="teaser_img_div"><a href="https://doi.org/10.1038/s41598-020-75120-7"><img class="teaser_img" src="images/2020_scireports.png" /></a></div></td>
  <td class="pub_td2"><b>Automated feature quantification of Lipiodol as imaging biomarker to predict therapeutic efficacy of conventional transarterial chemoembolization of liver cancer</b><br />
		<div class='paper_metadata'>
	  Sophie Stark, <u>Clinton Wang</u>,
	  <a href="https://radiologie.charite.de/en/metas/person/person/address_detail/savic/">Lynn Savic</a>,
	  <a href="https://www.linkedin.com/in/brian-letzen-m-d-m-s-74a364b/">Brian Letzen</a>,
	  Isabel Schobert, Milena Miszczuk, Nikitha Murali, Paula Oestmann, Bernhard Gebauer, 
		<a href="https://medicine.yale.edu/profile/mingde_lin/">MingDe Lin</a>,
		<a href="https://medicine.yale.edu/profile/james_duncan/">James Duncan</a>,
		<a href="https://medicine.yale.edu/profile/todd_schlachter/">Todd Schlachter</a>,
		<a href="https://medicine.yale.edu/profile/julius_chapiro/">Julius Chapiro</a><br />
	  <a href="https://www.nature.com/srep/about"><i>Scientific Reports</i></a> 2020<br />
		</div>
  <a href="https://doi.org/10.1038/s41598-020-75120-7">Paper</a> | <a href="https://github.com/clintonjwang/lipiodol">Code</a>
</td></tr>

<!-- <tr><td class="year_heading">2019<hr class="year_hr_wteaser"></td></tr> -->
<tr>
	<td class="pub_td1"><div class="teaser_img_div"><a href="https://doi.org/10.1117/12.2512473"><img class="teaser_img" src="images/spie19_teaser.jpg"/></a></div></td>
	<td class="pub_td2"><b>A probabilistic approach for interpretable deep learning in liver cancer diagnosis</b><br>
		<div class='paper_metadata'>
		<u>Clinton Wang</u>, Charlie Hamm,
	  <a href="https://www.linkedin.com/in/brian-letzen-m-d-m-s-74a364b/">Brian Letzen</a>,
		<a href="https://medicine.yale.edu/profile/james_duncan/">James Duncan</a><br>
		<a href="https://spie.org/conferences-and-exhibitions/medical-imaging?SSO=1"><i>SPIE Medical Imaging Conference</i></a> 2019<br>
		</div>
	<a href="https://doi.org/10.1117/12.2512473">Paper</a> | <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10950/2512473/A-probabilistic-approach-for-interpretable-deep-learning-in-liver-cancer/10.1117/12.2512473.full">Talk</a> | <a href="files/spie19_talk.pptx">Slides</a> | <a href="https://github.com/clintonjwang/voi-classifier">Code</a>
</td></tr>

<tr>
	<td class="pub_td1"><div class="teaser_img_div"><a href="https://doi.org/10.1007/s00330-019-06214-8"><img class="teaser_img" src="images/2020_eurorad_part2.png"/></a></div></td>
	<td class="pub_td2"><b>Deep learning for liver tumor diagnosis part II: interpretable deep learning to characterize tumor features</b><br>
		<div class='paper_metadata'>
		<u>Clinton Wang</u>*, Charlie Hamm*,
	  <a href="https://radiologie.charite.de/en/metas/person/person/address_detail/savic/">Lynn Savic</a>, Marc Ferrante, Isabel Schobert,
		<a href="https://medicine.yale.edu/profile/todd_schlachter/">Todd Schlachter</a>,
		<a href="https://medicine.yale.edu/profile/mingde_lin/">MingDe Lin</a>,
		<a href="https://medicine.yale.edu/profile/jeffrey_weinreb/">Jeffrey Weinreb</a>,
		<a href="https://medicine.yale.edu/profile/james_duncan/">James Duncan</a>,
		<a href="https://medicine.yale.edu/profile/julius_chapiro/">Julius Chapiro</a>,
	  <a href="https://www.linkedin.com/in/brian-letzen-m-d-m-s-74a364b/">Brian Letzen</a><br>
	  <a href="https://www.springer.com/journal/330"><i>European Radiology</i></a> 2019<br>
		</div>
	<a href="https://doi.org/10.1007/s00330-019-06214-8">Paper</a> | <a href="https://github.com/clintonjwang/voi-classifier/tree/part2">Code</a>
</td></tr>

<tr>
	<td class="pub_td1"><div class="teaser_img_div"><a href="https://doi.org/10.1007/s00330-019-06205-9"><img class="teaser_img" src="images/2020_eurorad_part1.png"/></a></div></td>
	<td class="pub_td2"><b>Deep learning for liver tumor diagnosis part I: development of a convolutional neural network classifier for multi-phasic MRI</b><br>
		<div class='paper_metadata'>
		Charlie Hamm*, <u>Clinton Wang</u>*, Marc Ferrante, Isabel Schobert,
		<a href="https://medicine.yale.edu/profile/todd_schlachter/">Todd Schlachter</a>,
		<a href="https://medicine.yale.edu/profile/mingde_lin/">MingDe Lin</a>, 
		<a href="https://medicine.yale.edu/profile/james_duncan/">James Duncan</a>, 
		<a href="https://medicine.yale.edu/profile/jeffrey_weinreb/">Jeffrey Weinreb</a>,
		<a href="https://medicine.yale.edu/profile/julius_chapiro/">Julius Chapiro</a>,
	  <a href="https://www.linkedin.com/in/brian-letzen-m-d-m-s-74a364b/">Brian Letzen</a><br>
	  <a href="https://www.springer.com/journal/330"><i>European Radiology</i></a> 2019<br>
		</div>
	<a href="https://doi.org/10.1007/s00330-019-06205-9">Paper</a> | <a href="https://github.com/clintonjwang/voi-classifier/tree/part1">Code</a>
</td></tr>


<tr>
	<td class="pub_td1"><div class="teaser_img_div"><a href="https://doi.org/10.1016/j.jvir.2018.08.032"><img class="teaser_img" src="images/jvir_review_teaser.jpg"/></a></div></td>
	<td class="pub_td2"><b>The Role of Artificial Intelligence in Interventional Oncology: A Primer</b><br>
		<div class='paper_metadata'>
	  <a href="https://www.linkedin.com/in/brian-letzen-m-d-m-s-74a364b/">Brian Letzen</a>,
	  <u>Clinton Wang</u>,
		<a href="https://medicine.yale.edu/profile/julius_chapiro/">Julius Chapiro</a>
		<br><i>Journal of Vascular and Interventional Radiology</i> 2019<br>
		</div>
	<a href="https://doi.org/10.1016/j.jvir.2018.08.032">Paper</a>
</td></tr>

<tr>
	<td class="pub_td1"><div class="teaser_img_div"><a href="https://doi.org/10.1016/j.yjmcc.2015.10.007"><img class="teaser_img" src="images/mybpc_jmcc.png"/></a></div></td>
	<td class="pub_td2"><b>Slowing of contractile kinetics by myosin-binding protein C can be explained by its cooperative binding to the thin filament</b><br>
		<div class='paper_metadata'>
		<u>Clinton Wang</u>, Jonas Schwan,
		<a href="https://seas.yale.edu/faculty-research/faculty-directory/stuart-campbell">Stuart Campbell</a>
		<br><i>Journal of Molecular and Cellular Cardiology</i> 2016<br>
		</div>
	<a href="https://doi.org/10.1016/j.yjmcc.2015.10.007">Paper</a>
</td></tr>
</table>


<h2>Teaching</h2>
<span style="font-size: 12pt;">
	Advances in Computer Vision (6.819/6.869), MIT, Spring 2021<br>
	<span style="font-size: 11pt;margin-left: 20px">
		<t>Teaching Assistant with Prof. <a href="https://billf.mit.edu/">Bill Freeman</a> and <a href="http://web.mit.edu/phillipi/">Phillip Isola</a><br>
	<span style="font-size: 12pt;">
	Undergraduate Mentor, MIT Undergraduate Research Opportunities Program, 2020<br>

<h2>Academic Service</h2>
<span style="font-size: 12pt;">
	Program Committee, Medical Imaging Meets NeurIPS Workshop (MedNeurIPS) 2022<br>
	Reviewer, Conference on Neural Information Processing Systems (NeurIPS) 2022<br>
	Reviewer, International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 2021-2022<br>

<h2>Selected Awards</h2>
<span style="font-size: 12pt;">
	Takeda Fellowship 2021-2022<br>
	Siebel Foundation Scholar 2020<br>
	Yale Department of Biomedical Engineering Prize, 2015<br>
	Tau Beta Pi Engineering Honor Society, 2015<br>
	
<h2>Invited Talks</h2>
<span style="font-size: 12pt;">
	MIT-Takeda Presentation Series 2022<br>
	MIT MGB AI Cures Conference 2022<br>
